{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models Performance\n",
    "\n",
    "## False Positives & Negatives\n",
    "\n",
    "\n",
    "Imagine o modelo linear abaixo e as observações 1,2,3 e 4:\n",
    "\n",
    "![](false_positives_negatives1.png)\n",
    "\n",
    "As observações em vermelhos são os pontos reais, e os em azul, os pontos previstos. Projetando as previsões, teríamos o seguinte resultado:\n",
    "\n",
    "![](false_positives_negatives2.png)\n",
    "\n",
    "Os pontos **1** e **4** foram previstos corretamente, já os pontos **2** e **3** não. O que aconteceu é que o ponto **2** ficou previsto como um **Falso Negativo** e o ponto **3** como **Falso Positivo**.\n",
    "\n",
    "Dependendo do tipo da análise, o mais perigido é o **Falso Negativo**, pois é mais grave dizer que algo não vai acontecer e ele acontece do que dizer que vai acontecer e não acontece.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "Imagine que vamos prever o resultado de 100 observações. Em 55% dos casos, o modelo previu como positivo e 45% como negativo, mas esse modelo não acertou 100% do resultado, houve um erro. Abaixo, na _confusion matrix_ é possível observar:\n",
    "\n",
    "![](confusion_matrix1.png)\n",
    "\n",
    "O primeiro quadrante nos diz a quantidade de observações que o modelo previu como _Negative_ e que realmente foram _Negative_, no caso *35*. O segundo quadrante informa a quantidade de observações que o modelo previu como _Positive_, mas que na verdade eram _Negative_. Temos *5* observações, e nesse caso, chamamos de _False Positive_. O terceiro quadrante diz a quantidade de items que o modelo previu como _Negative_, mas na verdade foram _Positive_, chamado de _False Negative_ (o mais perigoso), no caso *10* observações. O quarto e último quadrante diz a quantidade de observações que o modelo previu como _Positive_ e que realmente acertou, no caso *50*.\n",
    "\n",
    "Com esses números, podemos calcular algumas métricas para avaliar o nosso modelo. A primeira delas é a **acurácia** (*accuracy rate*):\n",
    "\n",
    "\n",
    "$$ Accuracy Rate = \\frac{Correct}{Total} $$\n",
    "\n",
    "$$ Accuracy Rate = \\frac{85}{100} = 85\\% $$\n",
    "\n",
    "Já o _Error Rate_ é a quantidade de itens previstos errados pelo total:\n",
    "\n",
    "$$ Error Rate = \\frac{Wrong}{Total} $$\n",
    "\n",
    "$$ Error Rate = \\frac{15}{100} = 15\\% $$\n",
    "\n",
    "\n",
    "## Accuracy Paradox\n",
    "\n",
    "Não podemos avaliar o desempenho de um modelo somente na acurácia. Imagina a matriz de confusão abaixo:\n",
    "\n",
    "<pre>\n",
    " |   0 |  1   |\n",
    "-|------------|\n",
    "0| 9700|  150 |\n",
    "-|------------|\n",
    "1|  50 |  100 |\n",
    "--------------|\n",
    "</pre>\n",
    "\n",
    "Canculando a acurácia, temos o seguinte:\n",
    "\n",
    "$$ Accuracy Rate = \\frac{9800}{10000} = 98\\% $$\n",
    "\n",
    "Agora imagine que abandonamos esse modelo e que todo valor previsto passa a ser *Negative*, ou seja, zero!\n",
    "\n",
    "<pre>\n",
    " |   0 |  1   |\n",
    "-|------------|\n",
    "0| 9850|  0   |\n",
    "-|------------|\n",
    "1|  150|  0   |\n",
    "--------------|\n",
    "</pre>\n",
    "\n",
    "A acurácia ficará:\n",
    "\n",
    "$$ Accuracy Rate = \\frac{9850}{10000} = 98,5\\% $$\n",
    "\n",
    "\n",
    "Não podemos avaliar somente por essa métrica, pois ela pode te enganar. Existem várias outras métricas que podem nos ajudar a avaliar o modelo.\n",
    "\n",
    "## (Cumulative accuracy profile) CAP Curve\n",
    "\n",
    "\n",
    "Imagine os modelos abaixo:\n",
    "\n",
    "![](cap_curve1.png)\n",
    "\n",
    "Esse é um modelo de propaganda, onde somente 10% dos contactados compram o produto. Temos um modelo aleatório, um modelo bom e o perfeito (que é praticamente impossível se obter).\n",
    "\n",
    "Uma firma de se calcular, é traçar uma linha em 50% e ver qual é o resultado no eixo _Purchased_.\n",
    "\n",
    "![](cap_curve2.png)\n",
    "\n",
    "Uma tabela nos diz qual é a qualidade do modelo. Uma observação aqui, é que se o resultado está entre 90% e 100%, o modelo pode ser bom demais para ser verdade e talvez tenha algum _overfitting_ aqui, que precisa ser analisado com mais cuidado.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
